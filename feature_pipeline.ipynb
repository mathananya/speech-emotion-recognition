{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":653195,"sourceType":"datasetVersion","datasetId":325566,"isSourceIdPinned":false}],"dockerImageVersionId":31234,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/ananyamkhrj/ser-project-feature-extraction-data-augmentation?scriptVersionId=292057016\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"86e1820f","cell_type":"markdown","source":"# Speech Emotion Recognition (SER) Project","metadata":{}},{"id":"4e9fb64c","cell_type":"code","source":"#!pip3 install librosa\n#!pip3 install numpy\n#!pip3 install kagglehub\n#!pip3 install IPython\n#!pip3 install matplotlib\n#!pip3 install pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:49.905477Z","iopub.execute_input":"2026-01-15T16:02:49.905773Z","iopub.status.idle":"2026-01-15T16:02:49.910373Z","shell.execute_reply.started":"2026-01-15T16:02:49.90575Z","shell.execute_reply":"2026-01-15T16:02:49.909334Z"}},"outputs":[],"execution_count":1},{"id":"bf3c91ef","cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport os\nimport kagglehub\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:49.912061Z","iopub.execute_input":"2026-01-15T16:02:49.912359Z","iopub.status.idle":"2026-01-15T16:02:51.73145Z","shell.execute_reply.started":"2026-01-15T16:02:49.912339Z","shell.execute_reply":"2026-01-15T16:02:51.73072Z"}},"outputs":[],"execution_count":2},{"id":"5ef5eb31","cell_type":"markdown","source":"## Feature Extraction","metadata":{}},{"id":"9d933b6d","cell_type":"markdown","source":"### 1. MFCCs\n\nMFCCs can be represented as an image since they are of the form of a 2D array, with one axis along time.\nOtherwise, we can take the mean of each MFCC across time, which might be faster to process for a real-time engine.<br>\n\nHere, we extract 16 MFCCs and return their means across time as a feature of the data.","metadata":{}},{"id":"f4c7ce59","cell_type":"code","source":"def extract_mfccs(data,sample_rate):\n    mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=15).T     # Extract 15 MFCCs of the form of a 2D array\n    # mfccs = np.mean(mfccs, axis=0)  # Take the mean of the MFCCs across time axis\n    # print(\"MFCCs shape:\", mfccs.shape)\n    return mfccs\n\ndef extract_chroma(data,sample_rate):\n    chroma = librosa.feature.chroma_stft(y=data, sr=sample_rate, n_chroma=12).T\n    # print(\"Chroma shape:\", chroma.shape)\n    return chroma\n\ndef extract_mel(data,sample_rate):\n    mel = librosa.feature.melspectrogram(y=data, sr=sample_rate, n_mels=128).T\n    # print(\"Mel Spectrogram shape:\", mel.shape)\n    return mel\n\n# example usage nutcracker through librosa example\n#filename = librosa.example('nutcracker')\n#y, sr = librosa.load(filename)\n#extract_mfccs(y, sr)\n#extract_chroma(y, sr)\n#extract_mel(y, sr)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.732293Z","iopub.execute_input":"2026-01-15T16:02:51.73262Z","iopub.status.idle":"2026-01-15T16:02:51.73812Z","shell.execute_reply.started":"2026-01-15T16:02:51.732601Z","shell.execute_reply":"2026-01-15T16:02:51.737188Z"}},"outputs":[],"execution_count":3},{"id":"94da9c51","cell_type":"markdown","source":"## Data Augmentation\n\n1. Noise Injection\n2. Shifting\n3. Pitch Stretching","metadata":{}},{"id":"dc58ffc5","cell_type":"markdown","source":"### 1. Noise Injection\n\nWe plan to implement **Gaussian Noise Injection** -- random noise with a probability density of a normal distribution.\n\nIt simulates microphone etc. sensor noise and is easy to implement.\n\nEnvironmental noise would be ideal, but we're ignoring it as its slightly difficult to randomise.\n\n_Pink/brown noise_ are more aligned to the human speech frequency distribution, and may provide better robustness when combined with white noise, however this will have to be tested.\n\nNoise will be adjusted on the basis of **SNR (Signal-to-Noise ratio)** of 25dB, so that noise is not overpowering.\n\n<center><pre>noise_power = signal_power / (10 ** (25/10)) = signal_power * 0.0032</pre></center>\n\nThe calculation <pre>(10**(25/10))</pre> converts SNR from dB to linear scale.\n\nThis can be changed later on to increase background noise.","metadata":{}},{"id":"ececcd69","cell_type":"code","source":"def noiseInjection(data):\n    signal_power = np.mean(data**2)\n    noise_power = signal_power * 0.0032     # calculated for an approximate SNR of 25 dB\n    noise = np.random.normal(0,np.sqrt(noise_power),len(data))\n    augmented_data = data + noise\n    return augmented_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.739092Z","iopub.execute_input":"2026-01-15T16:02:51.739325Z","iopub.status.idle":"2026-01-15T16:02:51.759353Z","shell.execute_reply.started":"2026-01-15T16:02:51.739302Z","shell.execute_reply":"2026-01-15T16:02:51.75857Z"}},"outputs":[],"execution_count":4},{"id":"cf0ced12","cell_type":"markdown","source":"### 2. Shifting\n\nSo, temporal translation (shifting) can be done by rolling the numpy arrays.\n\nShould the arrays only be rolled forward?<br>\nAlso, once rolled, should that portion of array must be set to silence (0)? This is a question regarding whether to use circular shift or zero-padding.<br>\n\nIn this case, we are rolling arrays in both directions and implementing circular shift of <10%\n","metadata":{}},{"id":"0b0a59e5","cell_type":"code","source":"def shifting(data):\n    shift = int(np.random.uniform(-0.1,0.1)*len(data))\n    augmented_data = np.roll(data,shift)\n    return augmented_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.760434Z","iopub.execute_input":"2026-01-15T16:02:51.760639Z","iopub.status.idle":"2026-01-15T16:02:51.778996Z","shell.execute_reply.started":"2026-01-15T16:02:51.760614Z","shell.execute_reply":"2026-01-15T16:02:51.778388Z"}},"outputs":[],"execution_count":5},{"id":"74750f2a","cell_type":"markdown","source":"### 3. Pitch Stretching\nrandomly changing the pitch using the librosa library Â±10 half-notes<br>\nPROBLEM -- if implementing pitch stretching, should we separate male and female voices?","metadata":{}},{"id":"b638b736","cell_type":"code","source":"def pitchStretching(data,sr):\n    pitch_stretch = np.random.uniform(-5,5)\n    augmented_data = librosa.effects.pitch_shift(data,sr=sr,n_steps=pitch_stretch)\n    return augmented_data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.779765Z","iopub.execute_input":"2026-01-15T16:02:51.780004Z","iopub.status.idle":"2026-01-15T16:02:51.7977Z","shell.execute_reply.started":"2026-01-15T16:02:51.779985Z","shell.execute_reply":"2026-01-15T16:02:51.796709Z"}},"outputs":[],"execution_count":6},{"id":"3461c9d6","cell_type":"markdown","source":"### Example of Data Augmentation\nAugmenting \"The Nutcracker\" with randomised noise injection, temporal shifting and pitch stretching.","metadata":{}},{"id":"5edf0bb3","cell_type":"code","source":"#y2 = pitchStretching(y,sr)\n#y2 = noiseInjection(y2)\n#y2 = shifting(y2)\n#ipd.display(ipd.Audio(y,rate=sr))\n#ipd.display(ipd.Audio(y2,rate=sr))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.800184Z","iopub.execute_input":"2026-01-15T16:02:51.800461Z","iopub.status.idle":"2026-01-15T16:02:51.812231Z","shell.execute_reply.started":"2026-01-15T16:02:51.800431Z","shell.execute_reply":"2026-01-15T16:02:51.811301Z"}},"outputs":[],"execution_count":7},{"id":"07cefe23","cell_type":"code","source":"# downloads RAVDESS dataset from kaggle\nRAVpath = kagglehub.dataset_download(\"uwrfkaggler/ravdess-emotional-speech-audio\")\nCREMApath = kagglehub.dataset_download(\"ejlok1/cremad\")\n\nprint(\"Path to dataset files:\", RAVpath)\nprint(\"Path to dataset files:\", CREMApath)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:51.813379Z","iopub.execute_input":"2026-01-15T16:02:51.81363Z","iopub.status.idle":"2026-01-15T16:02:52.382062Z","shell.execute_reply.started":"2026-01-15T16:02:51.81361Z","shell.execute_reply":"2026-01-15T16:02:52.380704Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/ravdess-emotional-speech-audio\nPath to dataset files: /kaggle/input/cremad\n","output_type":"stream"}],"execution_count":8},{"id":"12da8de2","cell_type":"code","source":"dirlist = os.listdir(RAVpath)\ndirlist.sort()\ndirlist2 = os.listdir(CREMApath)\ndirlist2.sort()\npd.set_option('display.max_columns', None)\n\n# first extracting filepaths of each subfolder in dirlist, and then extracting filepaths of each audio file in each subfolder\n# then, we extract emotion and gender from filenames and append to respective lists\n\nfilepaths = []\nemotions = []\ngenders = []\nfor subfolder in dirlist:\n    subfolder_path = os.path.join(RAVpath, subfolder)\n    if os.path.isdir(subfolder_path):\n        for file in os.listdir(subfolder_path):\n            codes = file.split('.')[0].split('-')\n            if file.endswith('.wav'):\n                filepaths.append(os.path.join(subfolder_path, file))\n                emotions.append(codes[2])\n                genders.append('female' if int(codes[6]) % 2 == 0 else 'male')\n            # print(filepaths[-3:])\n            # print(emotions[-3:])\n            # print(genders[-3:])\n\n\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,\n          1030,1037,1043,1046,1047,1049,1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,\n          1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor subfolder in dirlist2:\n    subfolder_path = os.path.join(CREMApath, subfolder)\n    if os.path.isdir(subfolder_path):\n        for file in os.listdir(subfolder_path):\n            codes = file.split('.')[0].split('_')\n            if file.endswith('.wav'):\n                filepaths.append(os.path.join(subfolder_path, file))\n                emotions.append(codes[2])\n                genders.append('female' if int(codes[0]) in female else 'male')\n\n# checking number of filepaths extracted\nprint(\"Total number of audio files:\", len(filepaths),len(emotions),len(genders))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:52.383232Z","iopub.execute_input":"2026-01-15T16:02:52.383572Z","iopub.status.idle":"2026-01-15T16:02:52.706332Z","shell.execute_reply.started":"2026-01-15T16:02:52.383539Z","shell.execute_reply":"2026-01-15T16:02:52.705296Z"}},"outputs":[{"name":"stdout","text":"Total number of audio files: 8882 8882 8882\n","output_type":"stream"}],"execution_count":9},{"id":"6de5c04d","cell_type":"code","source":"# creating pandas dataframe to store filepaths, emotions and genders\n\ndf = pd.DataFrame([filepaths, emotions, genders]).T\ndf.columns = ['filepath', 'emotion', 'gender']\nemotion_dict = {'01':'neutral','02':'calm','03':'happy','04':'sad','05':'angry','06':'fearful','07':'disgust','08':'surprised'}\nemotion_dict_crema = {'NEU':'neutral','HAP':'happy','SAD':'sad','ANG':'angry','FEA':'fearful','DIS':'disgust','SUR':'surprised'}\nemotion_dict.update(emotion_dict_crema)\ndf['emotion'] = df['emotion'].replace(emotion_dict)\ndf['label'] = df['gender'] + '-' + df['emotion']\ndf.drop(columns=['emotion','gender'], inplace=True)\nprint(df.head())\nprint(df.tail())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:52.70716Z","iopub.execute_input":"2026-01-15T16:02:52.707431Z","iopub.status.idle":"2026-01-15T16:02:52.886261Z","shell.execute_reply.started":"2026-01-15T16:02:52.707409Z","shell.execute_reply":"2026-01-15T16:02:52.885379Z"}},"outputs":[{"name":"stdout","text":"                                            filepath         label\n0  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm\n1  /kaggle/input/ravdess-emotional-speech-audio/A...  male-neutral\n2  /kaggle/input/ravdess-emotional-speech-audio/A...      male-sad\n3  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm\n4  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm\n                                               filepath         label\n8877  /kaggle/input/cremad/AudioWAV/1060_IEO_ANG_MD.wav  female-angry\n8878  /kaggle/input/cremad/AudioWAV/1088_IWL_ANG_XX.wav    male-angry\n8879  /kaggle/input/cremad/AudioWAV/1050_IOM_ANG_XX.wav    male-angry\n8880  /kaggle/input/cremad/AudioWAV/1044_IWL_SAD_XX.wav      male-sad\n8881  /kaggle/input/cremad/AudioWAV/1009_ITH_SAD_XX.wav    female-sad\n","output_type":"stream"}],"execution_count":10},{"id":"04f07c6c","cell_type":"code","source":"# adding feature columns to the dataframe\nmfccs = []\nchroma = []\nmel = []\nfor filepath in df['filepath']:\n    data, sample_rate = librosa.load(filepath)\n    mfccs.append(extract_mfccs(data, sample_rate))\n    chroma.append(extract_chroma(data, sample_rate))\n    mel.append(extract_mel(data, sample_rate))\n\nprint(\"MFCCs feature extraction completed:\", len(mfccs))\nprint(\"Chroma feature extraction completed:\", len(chroma))\nprint(\"Mel Spectrogram feature extraction completed:\", len(mel))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:02:52.887446Z","iopub.execute_input":"2026-01-15T16:02:52.887704Z","iopub.status.idle":"2026-01-15T16:07:58.806891Z","shell.execute_reply.started":"2026-01-15T16:02:52.887677Z","shell.execute_reply":"2026-01-15T16:07:58.805997Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n","output_type":"stream"},{"name":"stdout","text":"MFCCs feature extraction completed: 8882\nChroma feature extraction completed: 8882\nMel Spectrogram feature extraction completed: 8882\n","output_type":"stream"}],"execution_count":11},{"id":"9fc84211","cell_type":"code","source":"df['mfccs'] = mfccs\ndf['chroma'] = chroma\ndf['mel'] = mel\n\nprint(df.head())\ndf.to_csv('labels.csv', index=False)        # created a csv file with labels for each audio file\nnumpy_array = df.to_numpy()\nnp.save('labels.npy', numpy_array)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-15T16:07:58.807477Z","iopub.execute_input":"2026-01-15T16:07:58.807812Z","iopub.status.idle":"2026-01-15T16:08:08.075823Z","shell.execute_reply.started":"2026-01-15T16:07:58.807791Z","shell.execute_reply":"2026-01-15T16:08:08.075054Z"}},"outputs":[{"name":"stdout","text":"                                            filepath         label  \\\n0  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm   \n1  /kaggle/input/ravdess-emotional-speech-audio/A...  male-neutral   \n2  /kaggle/input/ravdess-emotional-speech-audio/A...      male-sad   \n3  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm   \n4  /kaggle/input/ravdess-emotional-speech-audio/A...     male-calm   \n\n                                               mfccs  \\\n0  [[-887.14105, 2.89413, 2.8835135, 2.8661366, 2...   \n1  [[-864.93823, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n2  [[-798.6087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...   \n3  [[-902.4752, 44.3454, 20.483978, 14.49259, 19....   \n4  [[-892.03625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0....   \n\n                                              chroma  \\\n0  [[0.85496235, 0.6903129, 0.70005983, 0.601727,...   \n1  [[0.727733, 0.9636807, 1.0, 0.80765074, 0.5434...   \n2  [[1.0, 0.66551745, 0.6343575, 0.6218832, 0.673...   \n3  [[0.8004069, 0.6972483, 0.7512276, 0.7488454, ...   \n4  [[0.74127454, 0.7179914, 0.8187605, 0.8978629,...   \n\n                                                 mel  \n0  [[4.9420542e-08, 1.4988275e-07, 7.14106e-08, 8...  \n1  [[2.5969036e-09, 5.2055933e-09, 4.1740886e-09,...  \n2  [[4.3227405e-10, 4.9346865e-10, 4.4047474e-10,...  \n3  [[1.5230578e-06, 5.603385e-06, 3.3235615e-06, ...  \n4  [[4.4146126e-10, 8.433001e-10, 5.2896665e-10, ...  \n","output_type":"stream"}],"execution_count":12}]}